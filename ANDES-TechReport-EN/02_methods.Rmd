# Methods
The \gls{ese} was the starting point in the development of \gls{andes}.
The replacement data entry software had to first replicate the proven functional capabilities of \gls{ese}.
Early versions of \gls{andes} achieved those goals and the system was further developed over its usage by scientists in the Gulf Region. 
Other opportunistic design goals also existed, including the use of a contemporary software development environment, the ability to support multiple languages, flexibility in deployment and using a shared code development environment.

<!-- 
Additionally, the \gls{mrr}
-->

## Technical Implementation of Design Goals

The following section outlines the user-requirements and design principles that played a role in decision-making during the development of \gls{andes}.

### Flexibility, Scalability and Reliability

The development environment used for the application must be a contemporary programming language with a proven track record for performance, usability and adaptability.
Within the context of DFO Science, there is a high number of use-cases under which this application can be deployed. For example, the number of users might range from a single employee alone in the field to several dozen scientists, technicians and vessel personnel participating in a research cruise. Similarly, the application might need to be deployed on a stand-alone device or accessed from within a \gls{lan} or even over a \gls{wan}. The application must be able to accommodate a wide range of practical scenarios.

### Version Control / Source Control

The utilization of a \gls{vcs} is an indispensable component of a sustainable development workflow. Version management is especially important in the context of having concurrent instances of the application in production at any given time. Knowing the version of a production instance is necessary to resolve any issues that might arise. Similarly, databases and backup files are intimately linked to an application's version number. In order to successfully re-instantiate a backup file, the precise version under which it was produced must be known. The VCS will also provide an indispensable framework for the coordination, examination and integration of contributions from collaborators.

### Unit Testing

The application performance needs to be reliable, especially considering its potential to be deployed in remote field environments. The implementation of unit tests is a practical way to ensure the maintenance of core functionality over time. At one extreme, the addition of any code can be preceded by the creation of unit tests (i.e., test-driven development). This approach will maximize the stability of an application, but can hinder the momentum of a project, especially in its early stages. At the other extreme, application development in the complete absence of unit testing occurs at a relatively fast pace but will result in a project that is vulnerable to breaking in unexpected ways and one that is difficult to maintain and pass on to other developers.

### Backup Strategy

In all deployment scenarios, data of high business value will be captured and there is little to no tolerance for data loss. Accordingly, the application must have a way to facilitate the implementation of a robust data-backup strategy. Capturing numerous snapshots of the application (and database) is ideal since doing so provides redundancy as well as the ability to revert to a specific point in time. If possible, The backup files and snapshots should be stored on a volumes with some form of redundancy.

### Customizable Protocols

The flexibility of project leads to design, modify and report on their sampling protocol without depending on developers is very important. The application should allow users to provide a variety of detailed information based on their particular sampling protocols, without the need to change the application source code. Similarly, different protocols utilize different code conventions for the identification of biological catches. The application should offer the flexibility of users to utilize their preferred system of catch codes.

### Quality Control

The implementation of quality control checks in a data entry application is of paramount importance. At a minimum, the following quality control checks should be leveraged:

\begin{description}
\item[Sets]{The application should ensure all the required fields on a given set card have been filled in. Users should also be warned if the set's start and/or end coordinates fall outside the expected sampling stratum (if applicable).}
\item[Catches]{The application should verify the validity of catches that do not have any data entry associated with them. This validation is important to help identify catches that might have been entered accidentally.}
\item[Specimens]{The application should flag specimens whose length falls outside an acceptable range. Similarly, a validation of the specimen's length-to-weight ratio should be performed.}
\item[Observations]{Individual observations are characterized by an observation type. Observation types should have predefined data types such as integer, float, string or categorical. The application should ensure that inputted observation values should never be left null and that they respect the data type of the corresponding observation type. In the case where an observation type (e.g., sex) has a set of defined categories (e.g., male, female, unknown), the application should ensure that any entered values fall within the set of available options.}
\end{description}

### User Interface

The user interface of the application can have a significant impact on user experience and onboarding. By ensuring the application has a modern and intuitive interface, the barriers related to onboarding new users are significantly reduced. Furthermore, an intuitive interface will reduce the need for extensive help documentation. Wherever extra annotation is required, documentation should be inserted directly in the application in the form of tool tips and help bubbles. By appealing to the end users' intuitions and by providing in-situ help documentation, we reduce the likelihood fields and features being used incorrectly.

### Reactivity

In the context of being on a research vessel survey, data entry happens at a very fast rate and on numerous devices; often with multiple transactions per second. Accordingly, it is imperative that the application does not create a bottleneck for data entry and is able to keep pace with experienced technicians. The usage of a reactive javascript library in conjunction with an \gls{api} would allow data entry to occur without webpages having to constantly refresh.

### Multilingualism

The ability for users to choose the language of their choice in the application is of considerable importance to this project. Previous tools that have been used were unilingual, and this by itself would have limited the scope of their use in a national context.


## Usage cases

A modularized application design would promote efficiency by allow multiple use-cases to leverage a core features and infrastructure such as authentication and authorization and many of the design elements listed above. The three major use cases that motivated this project are as follows.

### Ecosystem survey data entry

To be used as the main data entry system for ecosystem survey operations. This module should allow the capture of all the information required by the sampling protocols of the survey (e.g. @hurlbut-clay-1990). This entails the capture of all information related to fishing activities, including ancillary variables such as weather, sea state, water temperature. Catch contents are sorted and identified during scientific surveys, so the data capture application needs to support users in obtaining catch weights and abundance by taxon, individual observation of a specimen's length, weight, maturity status, etc. The module should also flag to the users when specimens are to be collected, or when additional sampling requirements are present (e.g. collecting fish otoliths).

### Oceanographic metadata collection

To be used to track and capture data and metadata from oceanographic operations such as \gls{ctd}s and zooplankton nets. These operations can be conducted on stand-alone oceanographic missions or may be collected as complementary data during biological surveys. In the case of the former, the coordination of oceanographic data with the associated biological data is a burdensome logistical challenge. Historically, the different types of data were captured and recorded using separate systems that operated in complete isolation from one another. Ideally, this module could streamline the capture of oceanographic data and simplify its association with the corresponding biological data.

### Commercial port sampling

To be used to support data collection during commercial port sampling activities [@Benoit-Daigle-2007]. This module is meant to be deployed on field tablets for scientific staff that collect data related to commercial fishing activities. A typical usage case is to obtain length frequency samples from commercial landings, and to also obtain length-stratified hard parts used for age estimation, and also the collection of whole individuals for later processing in the laboratory. It is not expected that there would be any interaction between this module and the ones listed above.
